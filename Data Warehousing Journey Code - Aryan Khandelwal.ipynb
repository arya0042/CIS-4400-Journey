{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1e833ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-storage-blob in c:\\users\\khand\\anaconda3\\lib\\site-packages (12.19.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from azure-storage-blob) (0.6.1)\n",
      "Requirement already satisfied: azure-core<2.0.0,>=1.28.0 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from azure-storage-blob) (1.29.5)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from azure-storage-blob) (37.0.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from azure-storage-blob) (4.8.0)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.28.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from cryptography>=2.1.4->azure-storage-blob) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\khand\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob) (2.21)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\khand\\anaconda3\\lib\\site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.28.0->azure-storage-blob) (3.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install azure-storage-blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9306ca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to Azure Storage Blob: journeydata/january_yellow_taxidata.parquet\n",
      "File uploaded to Azure Storage Blob: journeydata/january_green_taxidata.parquet\n",
      "File uploaded to Azure Storage Blob: journeydata/feburary_yellow_taxidata.parquet\n",
      "File uploaded to Azure Storage Blob: journeydata/feburary_green_taxidata.parquet\n",
      "File uploaded to Azure Storage Blob: journeydata/march_yellow_taxidata.parquet\n",
      "File uploaded to Azure Storage Blob: journeydata/march_green_taxidata.parquet\n",
      "File uploaded to Azure Storage Blob: journeydata/taxi_zone.csv\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import requests\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from io import BytesIO\n",
    "\n",
    "# Azure Storage Account details\n",
    "account_name = 'journeygroup'\n",
    "account_key = '2MVcmcztJuQBAB2QkioYpHDfWwQbYgMK4dCxpM59RKa2nUU4TtMs7eoDSymhiSbYdc9aBoFAoqhW+ASte7GP7g=='\n",
    "container_name = 'journeydata'\n",
    "\n",
    "# Function to upload Parquet file to Azure Storage Blob\n",
    "def upload_parquet_to_blob(parquet_url, blob_name):\n",
    "    # Download the Parquet file content\n",
    "    response = requests.get(parquet_url)\n",
    "    parquet_file_content = response.content\n",
    "\n",
    "    # Create a connection to Azure Storage Blob\n",
    "    blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "    # Upload the Parquet file to Azure Storage Blob\n",
    "    blob_client.upload_blob(parquet_file_content, overwrite=True)\n",
    "\n",
    "    print(f\"File uploaded to Azure Storage Blob: {blob_name}\")\n",
    "\n",
    "# Example: Upload the yellow_tripdata_2023-01.parquet file\n",
    "upload_parquet_to_blob('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet', 'journeydata/january_yellow_taxidata.parquet')\n",
    "upload_parquet_to_blob('https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-01.parquet', 'journeydata/january_green_taxidata.parquet')\n",
    "upload_parquet_to_blob('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-02.parquet', 'journeydata/feburary_yellow_taxidata.parquet')\n",
    "upload_parquet_to_blob('https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-02.parquet', 'journeydata/feburary_green_taxidata.parquet')\n",
    "upload_parquet_to_blob('https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet', 'journeydata/march_yellow_taxidata.parquet')\n",
    "upload_parquet_to_blob('https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2023-03.parquet', 'journeydata/march_green_taxidata.parquet')\n",
    "upload_parquet_to_blob('https://d37ci6vzurychx.cloudfront.net/misc/taxi+_zone_lookup.csv', 'journeydata/taxi_zone.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48f66ad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\january_yellow_taxidata.parquet\n",
      "File downloaded to: C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\feburary_yellow_taxidata.parquet\n",
      "File downloaded to: C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\march_yellow_taxidata.parquet\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Azure Storage Account details\n",
    "account_name = 'journeygroup'\n",
    "account_key = '2MVcmcztJuQBAB2QkioYpHDfWwQbYgMK4dCxpM59RKa2nUU4TtMs7eoDSymhiSbYdc9aBoFAoqhW+ASte7GP7g=='\n",
    "container_name = 'journeydata'\n",
    "\n",
    "# Function to download Parquet file from Azure Storage Blob\n",
    "def download_parquet_from_blob(blob_name, local_file_path):\n",
    "    # Create a connection to Azure Storage Blob\n",
    "    blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "    # Download the blob content\n",
    "    blob_data = blob_client.download_blob()\n",
    "    blob_content = blob_data.readall()\n",
    "\n",
    "    # Write the blob content to a local file\n",
    "    with open(local_file_path, 'wb') as local_file:\n",
    "        local_file.write(blob_content)\n",
    "\n",
    "    print(f\"File downloaded to: {local_file_path}\")\n",
    "\n",
    "# Example: Download the january_yellow_taxidata.parquet file\n",
    "download_parquet_from_blob('journeydata/january_yellow_taxidata.parquet', r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\january_yellow_taxidata.parquet')\n",
    "download_parquet_from_blob('journeydata/feburary_yellow_taxidata.parquet', r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\feburary_yellow_taxidata.parquet')\n",
    "download_parquet_from_blob('journeydata/march_yellow_taxidata.parquet', r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\march_yellow_taxidata.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e64424bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File downloaded to: C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\january_green_taxidata.parquet\n",
      "File downloaded to: C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\feburary_green_taxidata.parquet\n",
      "File downloaded to: C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\march_green_taxidata.parquet\n",
      "File downloaded to: C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\ID_Data.csv\n"
     ]
    }
   ],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Azure Storage Account details\n",
    "account_name = 'journeygroup'\n",
    "account_key = '2MVcmcztJuQBAB2QkioYpHDfWwQbYgMK4dCxpM59RKa2nUU4TtMs7eoDSymhiSbYdc9aBoFAoqhW+ASte7GP7g=='\n",
    "container_name = 'journeydata'\n",
    "\n",
    "# Function to download Parquet file from Azure Storage Blob\n",
    "def download_parquet_from_blob(blob_name, local_file_path):\n",
    "    # Create a connection to Azure Storage Blob\n",
    "    blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "\n",
    "    # Download the blob content\n",
    "    blob_data = blob_client.download_blob()\n",
    "    blob_content = blob_data.readall()\n",
    "\n",
    "    # Write the blob content to a local file\n",
    "    with open(local_file_path, 'wb') as local_file:\n",
    "        local_file.write(blob_content)\n",
    "\n",
    "    print(f\"File downloaded to: {local_file_path}\")\n",
    "\n",
    "# Example: Download the january_yellow_taxidata.parquet file\n",
    "download_parquet_from_blob('journeydata/january_green_taxidata.parquet', r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\january_green_taxidata.parquet')\n",
    "download_parquet_from_blob('journeydata/feburary_green_taxidata.parquet', r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\feburary_green_taxidata.parquet')\n",
    "download_parquet_from_blob('journeydata/march_green_taxidata.parquet', r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\march_green_taxidata.parquet')\n",
    "download_parquet_from_blob('journeydata/taxi_zone.csv', r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\ID_Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c155c5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>Taxi_Color</th>\n",
       "      <th>Borough</th>\n",
       "      <th>Zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-01-01 00:32:10</td>\n",
       "      <td>161</td>\n",
       "      <td>14.30</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-01-01 00:55:08</td>\n",
       "      <td>43</td>\n",
       "      <td>16.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Central Park</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-01-01 00:25:04</td>\n",
       "      <td>48</td>\n",
       "      <td>34.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Clinton East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-01-01 00:03:48</td>\n",
       "      <td>138</td>\n",
       "      <td>20.85</td>\n",
       "      <td>1</td>\n",
       "      <td>Queens</td>\n",
       "      <td>LaGuardia Airport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-01-01 00:10:29</td>\n",
       "      <td>107</td>\n",
       "      <td>19.68</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Gramercy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-01-01 00:50:34</td>\n",
       "      <td>161</td>\n",
       "      <td>27.80</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown Center</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-01-01 00:09:22</td>\n",
       "      <td>239</td>\n",
       "      <td>20.52</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Upper West Side South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-01-01 00:27:12</td>\n",
       "      <td>142</td>\n",
       "      <td>64.44</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Lincoln Square East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-01-01 00:21:44</td>\n",
       "      <td>164</td>\n",
       "      <td>28.38</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Midtown South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-01-01 00:39:42</td>\n",
       "      <td>141</td>\n",
       "      <td>19.90</td>\n",
       "      <td>1</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>Lenox Hill West</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tpep_pickup_datetime  PULocationID  total_amount  Taxi_Color    Borough  \\\n",
       "0  2023-01-01 00:32:10           161         14.30           1  Manhattan   \n",
       "1  2023-01-01 00:55:08            43         16.90           1  Manhattan   \n",
       "2  2023-01-01 00:25:04            48         34.90           1  Manhattan   \n",
       "3  2023-01-01 00:03:48           138         20.85           1     Queens   \n",
       "4  2023-01-01 00:10:29           107         19.68           1  Manhattan   \n",
       "5  2023-01-01 00:50:34           161         27.80           1  Manhattan   \n",
       "6  2023-01-01 00:09:22           239         20.52           1  Manhattan   \n",
       "7  2023-01-01 00:27:12           142         64.44           1  Manhattan   \n",
       "8  2023-01-01 00:21:44           164         28.38           1  Manhattan   \n",
       "9  2023-01-01 00:39:42           141         19.90           1  Manhattan   \n",
       "\n",
       "                    Zone  \n",
       "0         Midtown Center  \n",
       "1           Central Park  \n",
       "2           Clinton East  \n",
       "3      LaGuardia Airport  \n",
       "4               Gramercy  \n",
       "5         Midtown Center  \n",
       "6  Upper West Side South  \n",
       "7    Lincoln Square East  \n",
       "8          Midtown South  \n",
       "9        Lenox Hill West  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read location information from CSV\n",
    "file_path_location = r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\ID_Data.csv'\n",
    "location_info_df = pd.read_csv(file_path_location)\n",
    "\n",
    "\n",
    "# Read yellow taxi data\n",
    "file_path_yellow_jan = r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\january_yellow_taxidata.parquet'\n",
    "file_path_yellow_feb = r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\feburary_yellow_taxidata.parquet'\n",
    "file_path_yellow_march = r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\march_yellow_taxidata.parquet'\n",
    "\n",
    "df_yellow_jan = pd.read_parquet(file_path_yellow_jan)\n",
    "df_yellow_feb = pd.read_parquet(file_path_yellow_feb)\n",
    "df_yellow_march = pd.read_parquet(file_path_yellow_march)\n",
    "\n",
    "# Specify columns to remove from yellow taxi data\n",
    "columns_to_remove_yellow = [\n",
    "    'payment_type',\n",
    "    'fare_amount',\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'congestion_surcharge',\n",
    "    'airport_fee',\n",
    "    'passenger_count',\n",
    "    'tpep_dropoff_datetime',\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'store_and_fwd_flag',\n",
    "    'VendorID',\n",
    "    'DOLocationID'\n",
    "]\n",
    "\n",
    "# Drop specified columns from yellow taxi data\n",
    "for col in columns_to_remove_yellow:\n",
    "    if col in df_yellow_jan.columns:\n",
    "        df_yellow_jan = df_yellow_jan.drop(columns=col)\n",
    "    if col in df_yellow_feb.columns:\n",
    "        df_yellow_feb = df_yellow_feb.drop(columns=col)\n",
    "    if col in df_yellow_march.columns:\n",
    "        df_yellow_march = df_yellow_march.drop(columns=col)\n",
    "\n",
    "# Concatenate yellow taxi DataFrames vertically\n",
    "df_yellow = pd.concat([df_yellow_jan, df_yellow_feb, df_yellow_march], ignore_index=True)\n",
    "\n",
    "# Add a new column 'Taxi_Color' with each cell filled with 'Yellow'\n",
    "df_yellow['Taxi_Color'] = 1\n",
    "\n",
    "# Convert 'tpep_pickup_datetime' to datetime format if it's not already\n",
    "df_yellow['tpep_pickup_datetime'] = pd.to_datetime(df_yellow['tpep_pickup_datetime'])\n",
    "\n",
    "# Define the date range for yellow taxi\n",
    "start_date_yellow = '2023-01-01'\n",
    "end_date_yellow = '2023-03-31'\n",
    "\n",
    "# Filter out rows outside the specified date range for yellow taxi\n",
    "df_yellow = df_yellow[(df_yellow['tpep_pickup_datetime'] >= start_date_yellow) & (df_yellow['tpep_pickup_datetime'] <= end_date_yellow)]\n",
    "\n",
    "# Merge yellow taxi DataFrame with location information DataFrame\n",
    "df_yellow = pd.merge(df_yellow, location_info_df, left_on='PULocationID', right_on='LocationID', how='left')\n",
    "\n",
    "# Drop redundant columns\n",
    "df_yellow = df_yellow.drop(columns=['LocationID', 'Airport_fee', 'service_zone'])\n",
    "\n",
    "df_yellow.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb02bfce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tpep_pickup_datetime  PULocationID  total_amount  Taxi_Color    Borough  \\\n",
      "0  2023-01-01 00:32:10           161         14.30           1  Manhattan   \n",
      "1  2023-01-01 00:55:08            43         16.90           1  Manhattan   \n",
      "2  2023-01-01 00:25:04            48         34.90           1  Manhattan   \n",
      "3  2023-01-01 00:03:48           138         20.85           1     Queens   \n",
      "4  2023-01-01 00:10:29           107         19.68           1  Manhattan   \n",
      "5  2023-01-01 00:50:34           161         27.80           1  Manhattan   \n",
      "6  2023-01-01 00:09:22           239         20.52           1  Manhattan   \n",
      "7  2023-01-01 00:27:12           142         64.44           1  Manhattan   \n",
      "8  2023-01-01 00:21:44           164         28.38           1  Manhattan   \n",
      "9  2023-01-01 00:39:42           141         19.90           1  Manhattan   \n",
      "\n",
      "                    Zone  \n",
      "0         Midtown Center  \n",
      "1           Central Park  \n",
      "2           Clinton East  \n",
      "3      LaGuardia Airport  \n",
      "4               Gramercy  \n",
      "5         Midtown Center  \n",
      "6  Upper West Side South  \n",
      "7    Lincoln Square East  \n",
      "8          Midtown South  \n",
      "9        Lenox Hill West  \n"
     ]
    }
   ],
   "source": [
    "# Read green taxi data\n",
    "file_path_green_jan = r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\january_green_taxidata.parquet'\n",
    "file_path_green_feb = r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\feburary_green_taxidata.parquet'\n",
    "file_path_green_march = r'C:\\Users\\khand\\OneDrive\\Desktop\\STA 3000\\march_green_taxidata.parquet'\n",
    "\n",
    "df_green_jan = pd.read_parquet(file_path_green_jan)\n",
    "df_green_feb = pd.read_parquet(file_path_green_feb)\n",
    "df_green_march = pd.read_parquet(file_path_green_march)\n",
    "\n",
    "weather_data = pd.read_csv('C:/Users/khand/OneDrive/Desktop/STA 3000/weather_data.csv', parse_dates=['datetime'])\n",
    "\n",
    "# Specify columns to remove from green taxi data\n",
    "columns_to_remove_green = [\n",
    "    'payment_type',\n",
    "    'fare_amount',\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'congestion_surcharge',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'store_and_fwd_flag',\n",
    "    'VendorID',\n",
    "    'DOLocationID',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'ehail_fee',\n",
    "    'trip_type'\n",
    "]\n",
    "\n",
    "# Drop specified columns from green taxi data\n",
    "df_green_jan = df_green_jan.drop(columns=columns_to_remove_green)\n",
    "df_green_feb = df_green_feb.drop(columns=columns_to_remove_green)\n",
    "df_green_march = df_green_march.drop(columns=columns_to_remove_green)\n",
    "\n",
    "# Concatenate green taxi DataFrames vertically\n",
    "df_green = pd.concat([df_green_jan, df_green_feb, df_green_march], ignore_index=True)\n",
    "\n",
    "# Rename the 'lpep_pickup_datetime' column to 'tpep_pickup_datetime'\n",
    "df_green = df_green.rename(columns={'lpep_pickup_datetime': 'tpep_pickup_datetime'})\n",
    "\n",
    "# Add a new column 'Taxi_Color' with each cell filled with 'Green'\n",
    "df_green['Taxi_Color'] = 2\n",
    "\n",
    "# Convert 'tpep_pickup_datetime' to datetime format if it's not already\n",
    "df_green['tpep_pickup_datetime'] = pd.to_datetime(df_green['tpep_pickup_datetime'])\n",
    "\n",
    "# Define the date range for green taxi\n",
    "start_date_green = '2023-01-01'\n",
    "end_date_green = '2023-03-31'\n",
    "\n",
    "# Filter out rows outside the specified date range for green taxi\n",
    "df_green = df_green[(df_green['tpep_pickup_datetime'] >= start_date_green) & (df_green['tpep_pickup_datetime'] <= end_date_green)]\n",
    "\n",
    "# Reset the index after filtering for green taxi\n",
    "df_green.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Add a new column 'Taxi_Color' with each cell filled with 'Green'\n",
    "df_green['Taxi_Color'] = 2\n",
    "\n",
    "# Merge green taxi DataFrame with location information DataFrame\n",
    "df_green = pd.merge(df_green, location_info_df, left_on='PULocationID', right_on='LocationID', how='left')\n",
    "\n",
    "# Drop redundant columns for green taxi\n",
    "df_green = df_green.drop(columns=['LocationID'])\n",
    "\n",
    "# Concatenate yellow and green taxi DataFrames vertically\n",
    "df_combined = pd.concat([df_yellow, df_green], ignore_index=True)\n",
    "df_combined = df_combined.drop(columns=['service_zone'])\n",
    "\n",
    "# Remove rows with PULocationID of either 265 or 265\n",
    "df_combined = df_combined[(df_combined['PULocationID'] != 265) & (df_combined['PULocationID'] != 264)]\n",
    "\n",
    "# Reset index after removing rows\n",
    "df_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Now 'df_combined' contains rows with PULocationID not equal to 265 or 264\n",
    "print(df_combined.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73f8147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File uploaded to Azure Storage Blob: journeydata/LocationTable.csv\n",
      "File uploaded to Azure Storage Blob: journeydata/TaxiColorTable.csv\n",
      "File uploaded to Azure Storage Blob: journeydata/DateInfoTable.csv\n",
      "File uploaded to Azure Storage Blob: journeydata/TripsTable.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, ContainerClient\n",
    "from io import BytesIO\n",
    "\n",
    "# Azure Storage Account details\n",
    "account_name = 'journeygroup'\n",
    "account_key = '2MVcmcztJuQBAB2QkioYpHDfWwQbYgMK4dCxpM59RKa2nUU4TtMs7eoDSymhiSbYdc9aBoFAoqhW+ASte7GP7g=='\n",
    "container_name = 'journeydata'\n",
    "\n",
    "def save_to_csv_and_upload(data, csv_filename, blob_filename):\n",
    "    \"\"\"\n",
    "    Save DataFrame to CSV and upload to Azure Storage Blob.\n",
    "    \"\"\"\n",
    "    # Save to CSV\n",
    "    data.to_csv(csv_filename, index=False)\n",
    "\n",
    "    # Upload to Azure Storage Blob\n",
    "    upload_csv_to_blob(csv_filename, blob_filename)\n",
    "\n",
    "def upload_csv_to_blob(csv_filename, blob_filename):\n",
    "    \"\"\"\n",
    "    Upload CSV file to Azure Storage Blob.\n",
    "    \"\"\"\n",
    "    # Read CSV content\n",
    "    with open(csv_filename, 'rb') as file:\n",
    "        csv_bytes = file.read()\n",
    "\n",
    "    # Create a connection to Azure Storage Blob\n",
    "    blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "    blob_client = container_client.get_blob_client(blob_filename)\n",
    "\n",
    "    # Upload the CSV file to Azure Storage Blob\n",
    "    blob_client.upload_blob(csv_bytes, overwrite=True)\n",
    "\n",
    "    print(f\"File uploaded to Azure Storage Blob: {blob_filename}\")\n",
    "\n",
    "# Save Location table data to CSV\n",
    "location_table = location_info_df[['LocationID', 'Borough', 'Zone']].copy()\n",
    "location_table.columns = ['location_id', 'borough', 'zone']\n",
    "save_to_csv_and_upload(location_table, 'LocationTable.csv', 'journeydata/LocationTable.csv')\n",
    "\n",
    "# Save TaxiColor table data to CSV\n",
    "taxi_color_table = pd.DataFrame({'taxi_color_id': [1, 2], 'color_name': ['Yellow', 'Green']})\n",
    "save_to_csv_and_upload(taxi_color_table, 'TaxiColorTable.csv', 'journeydata/TaxiColorTable.csv')\n",
    "\n",
    "# Save DateInfo table data to CSV\n",
    "date_info_table = df_combined[['tpep_pickup_datetime']].copy()\n",
    "date_info_table['tpep_pickup_datetime'] = pd.to_datetime(date_info_table['tpep_pickup_datetime']).dt.date\n",
    "date_info_table = date_info_table.drop_duplicates().reset_index(drop=True)\n",
    "date_info_table.reset_index(inplace=True)\n",
    "date_info_table.columns = ['date_id', 'full_date']\n",
    "save_to_csv_and_upload(date_info_table, 'DateInfoTable.csv', 'journeydata/DateInfoTable.csv')\n",
    "\n",
    "# Save Trips table data to CSV\n",
    "trips_table = df_combined[['total_amount', 'Taxi_Color', 'PULocationID', 'tpep_pickup_datetime']].copy()\n",
    "trips_table.columns = ['total_amount', 'taxi_color_id', 'location_id', 'full_date']\n",
    "trips_table['full_date'] = pd.to_datetime(trips_table['full_date']).dt.date\n",
    "\n",
    "# Assuming date_info_table is available, merge to get date_id\n",
    "trips_table = pd.merge(trips_table, date_info_table, left_on='full_date', right_on='full_date', how='left')\n",
    "trips_table = trips_table[['total_amount', 'taxi_color_id', 'location_id', 'date_id']]\n",
    "\n",
    "# Add a new column 'trip_id' with sequential values starting from 1\n",
    "trips_table['trip_id'] = range(1, len(trips_table) + 1)\n",
    "\n",
    "# Reorder columns with 'trip_id' at the beginning\n",
    "trips_table = trips_table[['trip_id', 'total_amount', 'taxi_color_id', 'location_id', 'date_id']]\n",
    "\n",
    "save_to_csv_and_upload(trips_table, 'TripsTable.csv', 'journeydata/TripsTable.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5b9008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "# Your Azure Storage account information\n",
    "account_name = 'journeygroup'\n",
    "account_key = '2MVcmcztJuQBAB2QkioYpHDfWwQbYgMK4dCxpM59RKa2nUU4TtMs7eoDSymhiSbYdc9aBoFAoqhW+ASte7GP7g=='\n",
    "container_name = 'journeydata'\n",
    "\n",
    "# Load date_info_table and weather_data\n",
    "date_info_table = pd.read_csv('C:/Users/khand/OneDrive/Desktop/STA 3000/DateInfoTable.csv')\n",
    "weather_data = pd.read_csv('C:/Users/khand/OneDrive/Desktop/STA 3000/weather_data.csv')\n",
    "\n",
    "# Convert 'full_date' column in date_info_table to datetime\n",
    "date_info_table['full_date'] = pd.to_datetime(date_info_table['full_date'], format='%Y/%m/%d')\n",
    "\n",
    "# Merge each row in weather_data with the corresponding row in date_info_table\n",
    "merged_data = pd.merge(date_info_table, weather_data, left_index=True, right_index=True, suffixes=('_date_info', '_weather'))\n",
    "\n",
    "# Drop the 'datetime' column\n",
    "merged_data = merged_data.drop('datetime', axis=1)\n",
    "\n",
    "# Display the merged data\n",
    "print(merged_data)\n",
    "\n",
    "# Upload merged data to Azure Storage with the name \"DateInfoTable.csv\"\n",
    "blob_service_client = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=account_key)\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "# Define the file name in Azure Storage\n",
    "file_name = 'DateInfoTable.csv'\n",
    "\n",
    "# Convert DataFrame to CSV and upload to Azure Storage\n",
    "merged_data_csv = merged_data.to_csv(index=False)\n",
    "blob_client = container_client.get_blob_client(file_name)\n",
    "blob_client.upload_blob(merged_data_csv, overwrite=True)\n",
    "\n",
    "print(f'Merged data uploaded to Azure Storage as {file_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
